{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNEIOVf1Nu8biBkqTLspTgh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w1Uv-IBt0Vyk","executionInfo":{"status":"ok","timestamp":1701928098142,"user_tz":-420,"elapsed":10427,"user":{"displayName":"zulkaida akbar","userId":"03264843683657800588"}},"outputId":"4d9ecff3-4bb2-4c9e-b87c-6b4476186c9b"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["import json\n","import string\n","import random\n","import nltk\n","nltk.download('omw-1.4')\n","import numpy as num\n","from nltk.stem import WordNetLemmatizer # It has the ability to lemmatize.\n","import tensorflow as tensorF # A multidimensional array of elements is represented by this symbol.\n","from tensorflow.keras import Sequential # Sequential groups a linear stack of layers into a tf.keras.Model\n","from tensorflow.keras.layers import Dense, Dropout\n","\n","nltk.download(\"punkt\")# required package for tokenization\n","nltk.download(\"wordnet\")# word database"]},{"cell_type":"code","source":["ourData = {\"ourIntents\": [\n","\n","             {\"tag\": \"age\",\n","              \"patterns\": [\"how old are you?\"],\n","              \"responses\": [\"I am 2 years old and my birthday was yesterday\"]\n","             },\n","              {\"tag\": \"greeting\",\n","              \"patterns\": [ \"Hi\", \"Hello\", \"Hey\"],\n","              \"responses\": [\"Hi there\", \"Hello\", \"Hi :)\"],\n","             },\n","              {\"tag\": \"goodbye\",\n","              \"patterns\": [ \"bye\", \"later\"],\n","              \"responses\": [\"Bye\", \"take care\"]\n","             },\n","             {\"tag\": \"name\",\n","              \"patterns\": [\"what's your name?\", \"who are you?\"],\n","              \"responses\": [\"I have no name yet,\" \"You can give me one, and I will appreciate it\"]\n","             }\n","\n","]}"],"metadata":{"id":"mZLcJtX20-_m","executionInfo":{"status":"ok","timestamp":1701928267899,"user_tz":-420,"elapsed":328,"user":{"displayName":"zulkaida akbar","userId":"03264843683657800588"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["lm = WordNetLemmatizer() #for getting words\n","# lists\n","ourClasses = []\n","newWords = []\n","documentX = []\n","documentY = []\n","# Each intent is tokenized into words and the patterns and their associated tags are added to their respective lists.\n","for intent in ourData[\"ourIntents\"]:\n","    for pattern in intent[\"patterns\"]:\n","        ournewTkns = nltk.word_tokenize(pattern)# tokenize the patterns\n","        newWords.extend(ournewTkns)# extends the tokens\n","        documentX.append(pattern)\n","        documentY.append(intent[\"tag\"])\n","\n","\n","    if intent[\"tag\"] not in ourClasses:# add unexisting tags to their respective classes\n","        ourClasses.append(intent[\"tag\"])\n","\n","newWords = [lm.lemmatize(word.lower()) for word in newWords if word not in string.punctuation] # set words to lowercase if not in punctuation\n","newWords = sorted(set(newWords))# sorting words\n","ourClasses = sorted(set(ourClasses))# sorting classes"],"metadata":{"id":"zHZmojS71ObU","executionInfo":{"status":"ok","timestamp":1701928306462,"user_tz":-420,"elapsed":1987,"user":{"displayName":"zulkaida akbar","userId":"03264843683657800588"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["trainingData = [] # training list array\n","outEmpty = [0] * len(ourClasses)\n","# bow model\n","for idx, doc in enumerate(documentX):\n","    bagOfwords = []\n","    text = lm.lemmatize(doc.lower())\n","    for word in newWords:\n","        bagOfwords.append(1) if word in text else bagOfwords.append(0)\n","\n","    outputRow = list(outEmpty)\n","    outputRow[ourClasses.index(documentY[idx])] = 1\n","    trainingData.append([bagOfwords, outputRow])\n","\n","random.shuffle(trainingData)\n","trainingData = num.array(trainingData, dtype=object)# coverting our data into an array afterv shuffling\n","\n","x = num.array(list(trainingData[:, 0]))# first trainig phase\n","y = num.array(list(trainingData[:, 1]))# second training phase\n",""],"metadata":{"id":"nkPna6ts1Rjt","executionInfo":{"status":"ok","timestamp":1701928359573,"user_tz":-420,"elapsed":372,"user":{"displayName":"zulkaida akbar","userId":"03264843683657800588"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["iShape = (len(x[0]),)\n","oShape = len(y[0])\n","# parameter definition\n","ourNewModel = Sequential()\n","# In the case of a simple stack of layers, a Sequential model is appropriate\n","\n","# Dense function adds an output layer\n","ourNewModel.add(Dense(128, input_shape=iShape, activation=\"relu\"))\n","# The activation function in a neural network is in charge of converting the node's summed weighted input into activation of the node or output for the input in question\n","ourNewModel.add(Dropout(0.5))\n","# Dropout is used to enhance visual perception of input neurons\n","ourNewModel.add(Dense(64, activation=\"relu\"))\n","ourNewModel.add(Dropout(0.3))\n","ourNewModel.add(Dense(oShape, activation = \"softmax\"))\n","# below is a callable that returns the value to be used with no arguments\n","md = tensorF.keras.optimizers.Adam(learning_rate=0.005)\n","# Below line improves the numerical stability and pushes the computation of the probability distribution into the categorical crossentropy loss function.\n","ourNewModel.compile(loss='categorical_crossentropy',\n","              optimizer=md,\n","              metrics=[\"accuracy\"])\n","# Output the model in summary\n","print(ourNewModel.summary())\n","# Whilst training your Nural Network, you have the option of making the output verbose or simple.\n","ourNewModel.fit(x, y, epochs=200, verbose=1)\n","# By epochs, we mean the number of times you repeat a training set."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"koYg8fD51eBj","executionInfo":{"status":"ok","timestamp":1701928664552,"user_tz":-420,"elapsed":6198,"user":{"displayName":"zulkaida akbar","userId":"03264843683657800588"}},"outputId":"257a4970-5b73-44b0-c5b3-783b4ba77e3b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_3 (Dense)             (None, 128)               1920      \n","                                                                 \n"," dropout_2 (Dropout)         (None, 128)               0         \n","                                                                 \n"," dense_4 (Dense)             (None, 64)                8256      \n","                                                                 \n"," dropout_3 (Dropout)         (None, 64)                0         \n","                                                                 \n"," dense_5 (Dense)             (None, 4)                 260       \n","                                                                 \n","=================================================================\n","Total params: 10436 (40.77 KB)\n","Trainable params: 10436 (40.77 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","None\n","Epoch 1/200\n","1/1 [==============================] - 1s 980ms/step - loss: 1.3532 - accuracy: 0.0000e+00\n","Epoch 2/200\n","1/1 [==============================] - 0s 13ms/step - loss: 1.3575 - accuracy: 0.2500\n","Epoch 3/200\n","1/1 [==============================] - 0s 14ms/step - loss: 1.1928 - accuracy: 0.7500\n","Epoch 4/200\n","1/1 [==============================] - 0s 15ms/step - loss: 1.1921 - accuracy: 0.5000\n","Epoch 5/200\n","1/1 [==============================] - 0s 12ms/step - loss: 1.1498 - accuracy: 0.3750\n","Epoch 6/200\n","1/1 [==============================] - 0s 12ms/step - loss: 1.1086 - accuracy: 0.6250\n","Epoch 7/200\n","1/1 [==============================] - 0s 12ms/step - loss: 1.0848 - accuracy: 0.7500\n","Epoch 8/200\n","1/1 [==============================] - 0s 13ms/step - loss: 1.0462 - accuracy: 0.7500\n","Epoch 9/200\n","1/1 [==============================] - 0s 12ms/step - loss: 1.0038 - accuracy: 0.7500\n","Epoch 10/200\n","1/1 [==============================] - 0s 15ms/step - loss: 0.8967 - accuracy: 1.0000\n","Epoch 11/200\n","1/1 [==============================] - 0s 17ms/step - loss: 0.8455 - accuracy: 0.7500\n","Epoch 12/200\n","1/1 [==============================] - 0s 13ms/step - loss: 0.8037 - accuracy: 1.0000\n","Epoch 13/200\n","1/1 [==============================] - 0s 12ms/step - loss: 0.8287 - accuracy: 0.8750\n","Epoch 14/200\n","1/1 [==============================] - 0s 13ms/step - loss: 0.7016 - accuracy: 0.7500\n","Epoch 15/200\n","1/1 [==============================] - 0s 13ms/step - loss: 0.6774 - accuracy: 1.0000\n","Epoch 16/200\n","1/1 [==============================] - 0s 13ms/step - loss: 0.6189 - accuracy: 0.8750\n","Epoch 17/200\n","1/1 [==============================] - 0s 13ms/step - loss: 0.5037 - accuracy: 1.0000\n","Epoch 18/200\n","1/1 [==============================] - 0s 14ms/step - loss: 0.8444 - accuracy: 0.8750\n","Epoch 19/200\n","1/1 [==============================] - 0s 14ms/step - loss: 0.4100 - accuracy: 1.0000\n","Epoch 20/200\n","1/1 [==============================] - 0s 16ms/step - loss: 0.5325 - accuracy: 0.8750\n","Epoch 21/200\n","1/1 [==============================] - 0s 13ms/step - loss: 0.3602 - accuracy: 1.0000\n","Epoch 22/200\n","1/1 [==============================] - 0s 19ms/step - loss: 0.3821 - accuracy: 1.0000\n","Epoch 23/200\n","1/1 [==============================] - 0s 12ms/step - loss: 0.4955 - accuracy: 0.8750\n","Epoch 24/200\n","1/1 [==============================] - 0s 13ms/step - loss: 0.2831 - accuracy: 1.0000\n","Epoch 25/200\n","1/1 [==============================] - 0s 15ms/step - loss: 0.1586 - accuracy: 1.0000\n","Epoch 26/200\n","1/1 [==============================] - 0s 14ms/step - loss: 0.2936 - accuracy: 1.0000\n","Epoch 27/200\n","1/1 [==============================] - 0s 16ms/step - loss: 0.1377 - accuracy: 1.0000\n","Epoch 28/200\n","1/1 [==============================] - 0s 18ms/step - loss: 0.1474 - accuracy: 1.0000\n","Epoch 29/200\n","1/1 [==============================] - 0s 14ms/step - loss: 0.1562 - accuracy: 1.0000\n","Epoch 30/200\n","1/1 [==============================] - 0s 14ms/step - loss: 0.0830 - accuracy: 1.0000\n","Epoch 31/200\n","1/1 [==============================] - 0s 15ms/step - loss: 0.1780 - accuracy: 1.0000\n","Epoch 32/200\n","1/1 [==============================] - 0s 14ms/step - loss: 0.0653 - accuracy: 1.0000\n","Epoch 33/200\n","1/1 [==============================] - 0s 16ms/step - loss: 0.0827 - accuracy: 1.0000\n","Epoch 34/200\n","1/1 [==============================] - 0s 15ms/step - loss: 0.0419 - accuracy: 1.0000\n","Epoch 35/200\n","1/1 [==============================] - 0s 13ms/step - loss: 0.0506 - accuracy: 1.0000\n","Epoch 36/200\n","1/1 [==============================] - 0s 16ms/step - loss: 0.0271 - accuracy: 1.0000\n","Epoch 37/200\n","1/1 [==============================] - 0s 19ms/step - loss: 0.0663 - accuracy: 1.0000\n","Epoch 38/200\n","1/1 [==============================] - 0s 20ms/step - loss: 0.0548 - accuracy: 1.0000\n","Epoch 39/200\n","1/1 [==============================] - 0s 13ms/step - loss: 0.0373 - accuracy: 1.0000\n","Epoch 40/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0358 - accuracy: 1.0000\n","Epoch 41/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0372 - accuracy: 1.0000\n","Epoch 42/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0519 - accuracy: 1.0000\n","Epoch 43/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0176 - accuracy: 1.0000\n","Epoch 44/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0136 - accuracy: 1.0000\n","Epoch 45/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0073 - accuracy: 1.0000\n","Epoch 46/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0236 - accuracy: 1.0000\n","Epoch 47/200\n","1/1 [==============================] - 0s 13ms/step - loss: 0.0052 - accuracy: 1.0000\n","Epoch 48/200\n","1/1 [==============================] - 0s 13ms/step - loss: 0.0133 - accuracy: 1.0000\n","Epoch 49/200\n","1/1 [==============================] - 0s 16ms/step - loss: 0.0028 - accuracy: 1.0000\n","Epoch 50/200\n","1/1 [==============================] - 0s 16ms/step - loss: 0.0073 - accuracy: 1.0000\n","Epoch 51/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0154 - accuracy: 1.0000\n","Epoch 52/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0044 - accuracy: 1.0000\n","Epoch 53/200\n","1/1 [==============================] - 0s 12ms/step - loss: 0.0067 - accuracy: 1.0000\n","Epoch 54/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0312 - accuracy: 1.0000\n","Epoch 55/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0073 - accuracy: 1.0000\n","Epoch 56/200\n","1/1 [==============================] - 0s 14ms/step - loss: 0.0026 - accuracy: 1.0000\n","Epoch 57/200\n","1/1 [==============================] - 0s 13ms/step - loss: 0.0104 - accuracy: 1.0000\n","Epoch 58/200\n","1/1 [==============================] - 0s 12ms/step - loss: 0.0079 - accuracy: 1.0000\n","Epoch 59/200\n","1/1 [==============================] - 0s 12ms/step - loss: 0.0039 - accuracy: 1.0000\n","Epoch 60/200\n","1/1 [==============================] - 0s 12ms/step - loss: 0.0085 - accuracy: 1.0000\n","Epoch 61/200\n","1/1 [==============================] - 0s 15ms/step - loss: 0.0071 - accuracy: 1.0000\n","Epoch 62/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0138 - accuracy: 1.0000\n","Epoch 63/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0053 - accuracy: 1.0000\n","Epoch 64/200\n","1/1 [==============================] - 0s 11ms/step - loss: 5.0952e-04 - accuracy: 1.0000\n","Epoch 65/200\n","1/1 [==============================] - 0s 11ms/step - loss: 3.8266e-04 - accuracy: 1.0000\n","Epoch 66/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0031 - accuracy: 1.0000\n","Epoch 67/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0026 - accuracy: 1.0000\n","Epoch 68/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 1.0000\n","Epoch 69/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0013 - accuracy: 1.0000\n","Epoch 70/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0067 - accuracy: 1.0000\n","Epoch 71/200\n","1/1 [==============================] - 0s 13ms/step - loss: 5.0545e-04 - accuracy: 1.0000\n","Epoch 72/200\n","1/1 [==============================] - 0s 13ms/step - loss: 0.0015 - accuracy: 1.0000\n","Epoch 73/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0012 - accuracy: 1.0000\n","Epoch 74/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0069 - accuracy: 1.0000\n","Epoch 75/200\n","1/1 [==============================] - 0s 21ms/step - loss: 0.0117 - accuracy: 1.0000\n","Epoch 76/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0095 - accuracy: 1.0000\n","Epoch 77/200\n","1/1 [==============================] - 0s 13ms/step - loss: 0.0051 - accuracy: 1.0000\n","Epoch 78/200\n","1/1 [==============================] - 0s 13ms/step - loss: 0.0026 - accuracy: 1.0000\n","Epoch 79/200\n","1/1 [==============================] - 0s 12ms/step - loss: 0.0083 - accuracy: 1.0000\n","Epoch 80/200\n","1/1 [==============================] - 0s 14ms/step - loss: 0.0031 - accuracy: 1.0000\n","Epoch 81/200\n","1/1 [==============================] - 0s 13ms/step - loss: 0.0014 - accuracy: 1.0000\n","Epoch 82/200\n","1/1 [==============================] - 0s 10ms/step - loss: 9.5506e-04 - accuracy: 1.0000\n","Epoch 83/200\n","1/1 [==============================] - 0s 13ms/step - loss: 8.0420e-04 - accuracy: 1.0000\n","Epoch 84/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0020 - accuracy: 1.0000\n","Epoch 85/200\n","1/1 [==============================] - 0s 9ms/step - loss: 3.5515e-04 - accuracy: 1.0000\n","Epoch 86/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 1.0000\n","Epoch 87/200\n","1/1 [==============================] - 0s 12ms/step - loss: 0.0032 - accuracy: 1.0000\n","Epoch 88/200\n","1/1 [==============================] - 0s 12ms/step - loss: 7.6863e-04 - accuracy: 1.0000\n","Epoch 89/200\n","1/1 [==============================] - 0s 16ms/step - loss: 0.0028 - accuracy: 1.0000\n","Epoch 90/200\n","1/1 [==============================] - 0s 13ms/step - loss: 0.0112 - accuracy: 1.0000\n","Epoch 91/200\n","1/1 [==============================] - 0s 14ms/step - loss: 5.0805e-04 - accuracy: 1.0000\n","Epoch 92/200\n","1/1 [==============================] - 0s 15ms/step - loss: 0.0096 - accuracy: 1.0000\n","Epoch 93/200\n","1/1 [==============================] - 0s 12ms/step - loss: 0.0017 - accuracy: 1.0000\n","Epoch 94/200\n","1/1 [==============================] - 0s 18ms/step - loss: 6.9079e-04 - accuracy: 1.0000\n","Epoch 95/200\n","1/1 [==============================] - 0s 15ms/step - loss: 7.7716e-04 - accuracy: 1.0000\n","Epoch 96/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0087 - accuracy: 1.0000\n","Epoch 97/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0023 - accuracy: 1.0000\n","Epoch 98/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0115 - accuracy: 1.0000\n","Epoch 99/200\n","1/1 [==============================] - 0s 11ms/step - loss: 9.4409e-04 - accuracy: 1.0000\n","Epoch 100/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0014 - accuracy: 1.0000\n","Epoch 101/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0036 - accuracy: 1.0000\n","Epoch 102/200\n","1/1 [==============================] - 0s 12ms/step - loss: 0.0014 - accuracy: 1.0000\n","Epoch 103/200\n","1/1 [==============================] - 0s 10ms/step - loss: 5.5224e-04 - accuracy: 1.0000\n","Epoch 104/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0066 - accuracy: 1.0000\n","Epoch 105/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0055 - accuracy: 1.0000\n","Epoch 106/200\n","1/1 [==============================] - 0s 17ms/step - loss: 0.0074 - accuracy: 1.0000\n","Epoch 107/200\n","1/1 [==============================] - 0s 15ms/step - loss: 2.8800e-04 - accuracy: 1.0000\n","Epoch 108/200\n","1/1 [==============================] - 0s 13ms/step - loss: 7.2267e-04 - accuracy: 1.0000\n","Epoch 109/200\n","1/1 [==============================] - 0s 14ms/step - loss: 8.9597e-04 - accuracy: 1.0000\n","Epoch 110/200\n","1/1 [==============================] - 0s 9ms/step - loss: 8.7914e-04 - accuracy: 1.0000\n","Epoch 111/200\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0046 - accuracy: 1.0000\n","Epoch 112/200\n","1/1 [==============================] - 0s 10ms/step - loss: 2.8253e-04 - accuracy: 1.0000\n","Epoch 113/200\n","1/1 [==============================] - 0s 15ms/step - loss: 0.0200 - accuracy: 1.0000\n","Epoch 114/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0011 - accuracy: 1.0000\n","Epoch 115/200\n","1/1 [==============================] - 0s 10ms/step - loss: 5.2448e-04 - accuracy: 1.0000\n","Epoch 116/200\n","1/1 [==============================] - 0s 10ms/step - loss: 2.8094e-04 - accuracy: 1.0000\n","Epoch 117/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0016 - accuracy: 1.0000\n","Epoch 118/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0011 - accuracy: 1.0000\n","Epoch 119/200\n","1/1 [==============================] - 0s 10ms/step - loss: 8.2971e-04 - accuracy: 1.0000\n","Epoch 120/200\n","1/1 [==============================] - 0s 11ms/step - loss: 3.6820e-04 - accuracy: 1.0000\n","Epoch 121/200\n","1/1 [==============================] - 0s 12ms/step - loss: 5.1692e-04 - accuracy: 1.0000\n","Epoch 122/200\n","1/1 [==============================] - 0s 11ms/step - loss: 4.9682e-04 - accuracy: 1.0000\n","Epoch 123/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0042 - accuracy: 1.0000\n","Epoch 124/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0013 - accuracy: 1.0000\n","Epoch 125/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0015 - accuracy: 1.0000\n","Epoch 126/200\n","1/1 [==============================] - 0s 11ms/step - loss: 3.4959e-04 - accuracy: 1.0000\n","Epoch 127/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0010 - accuracy: 1.0000\n","Epoch 128/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0029 - accuracy: 1.0000\n","Epoch 129/200\n","1/1 [==============================] - 0s 10ms/step - loss: 6.1946e-04 - accuracy: 1.0000\n","Epoch 130/200\n","1/1 [==============================] - 0s 11ms/step - loss: 7.6116e-04 - accuracy: 1.0000\n","Epoch 131/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0032 - accuracy: 1.0000\n","Epoch 132/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0017 - accuracy: 1.0000\n","Epoch 133/200\n","1/1 [==============================] - 0s 12ms/step - loss: 8.4698e-04 - accuracy: 1.0000\n","Epoch 134/200\n","1/1 [==============================] - 0s 11ms/step - loss: 1.9628e-04 - accuracy: 1.0000\n","Epoch 135/200\n","1/1 [==============================] - 0s 12ms/step - loss: 4.8094e-04 - accuracy: 1.0000\n","Epoch 136/200\n","1/1 [==============================] - 0s 12ms/step - loss: 1.4981e-04 - accuracy: 1.0000\n","Epoch 137/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0017 - accuracy: 1.0000\n","Epoch 138/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0043 - accuracy: 1.0000\n","Epoch 139/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0179 - accuracy: 1.0000\n","Epoch 140/200\n","1/1 [==============================] - 0s 12ms/step - loss: 0.0018 - accuracy: 1.0000\n","Epoch 141/200\n","1/1 [==============================] - 0s 14ms/step - loss: 0.0010 - accuracy: 1.0000\n","Epoch 142/200\n","1/1 [==============================] - 0s 12ms/step - loss: 0.0031 - accuracy: 1.0000\n","Epoch 143/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0177 - accuracy: 1.0000\n","Epoch 144/200\n","1/1 [==============================] - 0s 10ms/step - loss: 5.8783e-04 - accuracy: 1.0000\n","Epoch 145/200\n","1/1 [==============================] - 0s 11ms/step - loss: 8.8240e-04 - accuracy: 1.0000\n","Epoch 146/200\n","1/1 [==============================] - 0s 11ms/step - loss: 4.7751e-04 - accuracy: 1.0000\n","Epoch 147/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0024 - accuracy: 1.0000\n","Epoch 148/200\n","1/1 [==============================] - 0s 11ms/step - loss: 5.9230e-04 - accuracy: 1.0000\n","Epoch 149/200\n","1/1 [==============================] - 0s 11ms/step - loss: 3.2471e-04 - accuracy: 1.0000\n","Epoch 150/200\n","1/1 [==============================] - 0s 10ms/step - loss: 8.1424e-04 - accuracy: 1.0000\n","Epoch 151/200\n","1/1 [==============================] - 0s 11ms/step - loss: 3.0733e-04 - accuracy: 1.0000\n","Epoch 152/200\n","1/1 [==============================] - 0s 13ms/step - loss: 4.4985e-04 - accuracy: 1.0000\n","Epoch 153/200\n","1/1 [==============================] - 0s 13ms/step - loss: 1.0324e-04 - accuracy: 1.0000\n","Epoch 154/200\n","1/1 [==============================] - 0s 10ms/step - loss: 9.7514e-04 - accuracy: 1.0000\n","Epoch 155/200\n","1/1 [==============================] - 0s 13ms/step - loss: 2.8062e-04 - accuracy: 1.0000\n","Epoch 156/200\n","1/1 [==============================] - 0s 10ms/step - loss: 5.5606e-05 - accuracy: 1.0000\n","Epoch 157/200\n","1/1 [==============================] - 0s 10ms/step - loss: 4.2886e-04 - accuracy: 1.0000\n","Epoch 158/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0017 - accuracy: 1.0000\n","Epoch 159/200\n","1/1 [==============================] - 0s 12ms/step - loss: 0.0035 - accuracy: 1.0000\n","Epoch 160/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0016 - accuracy: 1.0000\n","Epoch 161/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0042 - accuracy: 1.0000\n","Epoch 162/200\n","1/1 [==============================] - 0s 11ms/step - loss: 5.0780e-04 - accuracy: 1.0000\n","Epoch 163/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0013 - accuracy: 1.0000\n","Epoch 164/200\n","1/1 [==============================] - 0s 11ms/step - loss: 2.7749e-04 - accuracy: 1.0000\n","Epoch 165/200\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0014 - accuracy: 1.0000\n","Epoch 166/200\n","1/1 [==============================] - 0s 11ms/step - loss: 6.9830e-04 - accuracy: 1.0000\n","Epoch 167/200\n","1/1 [==============================] - 0s 10ms/step - loss: 2.1992e-04 - accuracy: 1.0000\n","Epoch 168/200\n","1/1 [==============================] - 0s 12ms/step - loss: 0.0042 - accuracy: 1.0000\n","Epoch 169/200\n","1/1 [==============================] - 0s 10ms/step - loss: 9.8019e-04 - accuracy: 1.0000\n","Epoch 170/200\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0076 - accuracy: 1.0000\n","Epoch 171/200\n","1/1 [==============================] - 0s 9ms/step - loss: 1.6826e-04 - accuracy: 1.0000\n","Epoch 172/200\n","1/1 [==============================] - 0s 9ms/step - loss: 3.7671e-04 - accuracy: 1.0000\n","Epoch 173/200\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0014 - accuracy: 1.0000\n","Epoch 174/200\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0011 - accuracy: 1.0000\n","Epoch 175/200\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0031 - accuracy: 1.0000\n","Epoch 176/200\n","1/1 [==============================] - 0s 10ms/step - loss: 2.0076e-04 - accuracy: 1.0000\n","Epoch 177/200\n","1/1 [==============================] - 0s 10ms/step - loss: 2.0593e-04 - accuracy: 1.0000\n","Epoch 178/200\n","1/1 [==============================] - 0s 10ms/step - loss: 6.4886e-04 - accuracy: 1.0000\n","Epoch 179/200\n","1/1 [==============================] - 0s 9ms/step - loss: 4.6772e-04 - accuracy: 1.0000\n","Epoch 180/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0016 - accuracy: 1.0000\n","Epoch 181/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0074 - accuracy: 1.0000\n","Epoch 182/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0139 - accuracy: 1.0000\n","Epoch 183/200\n","1/1 [==============================] - 0s 10ms/step - loss: 4.1165e-04 - accuracy: 1.0000\n","Epoch 184/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0032 - accuracy: 1.0000\n","Epoch 185/200\n","1/1 [==============================] - 0s 10ms/step - loss: 6.3592e-04 - accuracy: 1.0000\n","Epoch 186/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0013 - accuracy: 1.0000\n","Epoch 187/200\n","1/1 [==============================] - 0s 10ms/step - loss: 2.2442e-04 - accuracy: 1.0000\n","Epoch 188/200\n","1/1 [==============================] - 0s 10ms/step - loss: 8.3199e-04 - accuracy: 1.0000\n","Epoch 189/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0012 - accuracy: 1.0000\n","Epoch 190/200\n","1/1 [==============================] - 0s 10ms/step - loss: 9.6447e-05 - accuracy: 1.0000\n","Epoch 191/200\n","1/1 [==============================] - 0s 10ms/step - loss: 5.7595e-04 - accuracy: 1.0000\n","Epoch 192/200\n","1/1 [==============================] - 0s 14ms/step - loss: 3.8810e-04 - accuracy: 1.0000\n","Epoch 193/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0039 - accuracy: 1.0000\n","Epoch 194/200\n","1/1 [==============================] - 0s 10ms/step - loss: 5.3570e-04 - accuracy: 1.0000\n","Epoch 195/200\n","1/1 [==============================] - 0s 11ms/step - loss: 3.0799e-05 - accuracy: 1.0000\n","Epoch 196/200\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0220 - accuracy: 1.0000\n","Epoch 197/200\n","1/1 [==============================] - 0s 13ms/step - loss: 1.6577e-04 - accuracy: 1.0000\n","Epoch 198/200\n","1/1 [==============================] - 0s 10ms/step - loss: 3.5865e-05 - accuracy: 1.0000\n","Epoch 199/200\n","1/1 [==============================] - 0s 10ms/step - loss: 9.0691e-04 - accuracy: 1.0000\n","Epoch 200/200\n","1/1 [==============================] - 0s 10ms/step - loss: 2.2221e-04 - accuracy: 1.0000\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7b7e28e6bd30>"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["def ourText(text):\n","  newtkns = nltk.word_tokenize(text)\n","  newtkns = [lm.lemmatize(word) for word in newtkns]\n","  return newtkns\n","\n","def wordBag(text, vocab):\n","  newtkns = ourText(text)\n","  bagOwords = [0] * len(vocab)\n","  for w in newtkns:\n","    for idx, word in enumerate(vocab):\n","      if word == w:\n","        bagOwords[idx] = 1\n","  return num.array(bagOwords)\n","\n","def Pclass(text, vocab, labels):\n","  bagOwords = wordBag(text, vocab)\n","  ourResult = ourNewModel.predict(num.array([bagOwords]))[0]\n","  newThresh = 0.2\n","  yp = [[idx, res] for idx, res in enumerate(ourResult) if res > newThresh]\n","\n","  yp.sort(key=lambda x: x[1], reverse=True)\n","  newList = []\n","  for r in yp:\n","    newList.append(labels[r[0]])\n","  return newList\n","\n","def getRes(firstlist, fJson):\n","  tag = firstlist[0]\n","  listOfIntents = fJson[\"ourIntents\"]\n","  for i in listOfIntents:\n","    if i[\"tag\"] == tag:\n","      ourResult = random.choice(i[\"responses\"])\n","      break\n","  return ourResult"],"metadata":{"id":"3f7OXPj43o9S","executionInfo":{"status":"ok","timestamp":1701928934193,"user_tz":-420,"elapsed":309,"user":{"displayName":"zulkaida akbar","userId":"03264843683657800588"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["while True:\n","    newMessage = input(\"\")\n","    intents = Pclass(newMessage, newWords, ourClasses)\n","    ourResult = getRes(intents, ourData)\n","    print(ourResult)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":495},"id":"S8IvKNQ73bm6","executionInfo":{"status":"error","timestamp":1701928970677,"user_tz":-420,"elapsed":34265,"user":{"displayName":"zulkaida akbar","userId":"03264843683657800588"}},"outputId":"e0dc9ef4-26c9-415d-d71f-bb0b5ca85ac6"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Hi\n","1/1 [==============================] - 0s 101ms/step\n","Hi there\n","How old are you\n","1/1 [==============================] - 0s 32ms/step\n","I am 2 years old and my birthday was yesterday\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-9d0458adb0f2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mnewMessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mintents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewMessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewWords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mourClasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mourResult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetRes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mourData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mourResult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}]}]}